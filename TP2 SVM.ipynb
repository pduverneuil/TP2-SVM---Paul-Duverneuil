# TP2-SVM-Paul-Duverneuil

### Question 1) ###


from sklearn import datasets
from sklearn.svm import SVC
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import numpy as np
from sklearn.model_selection import train_test_split

iris = datasets.load_iris()

"""This data sets consists of 3 different types of irisesâ€™ 
(Setosa, Versicolour, and Virginica) petal and sepal length, 
stored in a 150x4 numpy.ndarray
The rows being the samples and the columns being: 
Sepal Length, Sepal Width, Petal Length and Petal Width."""


X = iris.data
y = iris.target

#We only want classes 1 & 2, and consider only the first 2 features.
X = X[y != 0, :2]
y = y[y != 0]

#Shuffling the data:
permutation = np.random.permutation(len(X))
X = X[permutation]
y = y[permutation]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5)

# fits the model with linear kernel
clf_lin = SVC(kernel='linear')
clf_lin.fit(X_train, y_train)

# predicts labels for the test data base
y_pred = clf_lin.predict(X_test)

# checks your score
score = clf_lin.score(X_test, y_test)
print('Score with linear kernel: %s' % score)


### Question 2) ###


# fits the model with linear kernel
clf_lin = SVC(kernel='linear')
clf_lin.fit(X_train, y_train)

# predicts labels for the test data base
y_pred = clf_lin.predict(X_test)

# checks your score
score = clf_lin.score(X_test, y_test)
print('Score with linear kernel: %s' % score)


### Question 5) ###


from sklearn.svm import SVC
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from os import listdir
import cv2

# to change if the image folder is somewhere else:
faces_path = "lfw/"

available_names = listdir(faces_path)
print("Available names: %s" % available_names)

import random
names = random.sample(available_names, 2)

print("Faces selected: %s" % names)

# Selecting all the images we can
images0 = [faces_path + names[0] + "/" + f for f in listdir(faces_path + names[0])]
images1 = [faces_path + names[1] + "/" + f for f in listdir(faces_path + names[1])]
# Selecting the same number of images for each person:
min_files = min(len(images0), len(images1))
print("Number of files selected : %s for each face." % min_files)
images0 = images0[:min_files]
images1 = images1[:min_files]

filenames = np.array(images0 + images1)
y = np.array(min_files * [0] + min_files * [+1])
X = []
for f in filenames:
    img = cv2.imread(f, cv2.IMREAD_GRAYSCALE) #matrix of grey values
    X.append(img.reshape(len(img)*len(img[0])))
X = np.array(X, dtype='float64')

# Shuffling the data:
permutation = np.random.permutation(2*min_files)
filenames = filenames[permutation]
y = y[permutation]
X = X[permutation]


# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

X_train, X_test, y_train, y_test, filenames_train, filenames_test = train_test_split(
    X, y, filenames, test_size=0.25)

import matplotlib.pyplot as plt
%matplotlib inline

# can be changed by the user:
low_exp  = -6
high_exp = 0
points   = 20
# Plot `points` points from 1^(`low_exp`) to 1^(`high_exp`) 

clf_face_lin = GridSearchCV(SVC(C=1), {'C':np.logspace(low_exp, high_exp, num=points), 'kernel':['linear']}, n_jobs=2)
clf_face_lin.fit(X_train, y_train)

C_params = [param['C'] for param in clf_face_lin.cv_results_['params']]
error   = 100 - 100*clf_face_lin.cv_results_['mean_test_score']
    
ax = plt.gca()
ax.scatter(C_params, error)
ax.set_xscale('log')
ax.set_xlim(np.min(C_params)/10, np.max(C_params)*10)
plt.title("Influence of penalty parameter")
plt.ylabel("Error percentage")
plt.xlabel("Penalty parameter")


### Question 6) ###


# Can be changed by the user:
high_exp = 4
# This cell tests for 10^`high_exp` parasitic variables

from IPython import display


X_true = X
scores = []
parasites_values = np.logspace(0,high_exp, num=15, dtype='int')

for n_parasite in parasites_values:
    X = X_true # Resetting X
    #Inserting random vectors:
    parasite = np.random.normal(0,1, size=(2*min_files, n_parasite))
    X = np.insert(X, np.random.randint(0, 100*100, n_parasite), parasite, axis=1)

    X_train, X_test, y_train, y_test, filenames_train, filenames_test = train_test_split(
        X, y, filenames, test_size=0.25)


    # fits the model with linear kernel
    clf_parasite = SVC(kernel='linear')
    clf_parasite.fit(X_train, y_train)

    # predicts labels for the test data base
    y_pred = clf_parasite.predict(X_test)

    # displays score
    score = 100*clf_parasite.score(X_test, y_test)
    scores.append(score)
    
    display.clear_output()
    ax = plt.gca()
    ax.set_xscale('log')
    plt.xlim(np.min(parasites_values)/10., np.max(parasites_values)*10)
    plt.ylim(-1, 101)
    plt.title("Influence of parasitic features")
    plt.ylabel("Score percentage")
    plt.xlabel("Number of parasitic features")
    ax.scatter(parasites_values[:len(scores)], scores)
    plt.show()
    
print("Done.")

#resetting X:
X = X_true


### Question 8) ###


# Can be changed by the user:
low_exp  = -2
high_exp = 1
points   = 20
# Plots `points` points from 1^(`low_exp`) to 1^(`high_exp`) 

C_params = np.logspace(low_exp, high_exp, num=points)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

clf_face_rbf = GridSearchCV(SVC(C=1), {'C':C_params, 'kernel':['rbf']}, n_jobs=2)
clf_face_rbf.fit(X_train, y_train)

error   = 100 - 100*clf_face_rbf.cv_results_['mean_test_score']
    
ax = plt.gca()
ax.scatter(C_params, error)
ax.set_xscale('log')
ax.set_xlim(np.min(C_params)/10, np.max(C_params)*10)
plt.title("Influence of penalty parameter")
plt.ylabel("Error percentage")
plt.xlabel("Penalty parameter")

from sklearn.decomposition import PCA

min_err = 100
min_c = -1
min_i = -1
for i in range(2,100,5):
    print ".",
    pca = PCA(n_components=i)

#     print("Dimension before PCA: ",X.shape)
    X_new = pca.fit_transform(X)
#     print("Dimension after PCA: ",X_new.shape)


    # Scale features
    X_new -= np.mean(X_new, axis=0)
    X_new /= np.std(X_new, axis=0)

    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.25)

    # Can be changed by the user:
    low_exp  = -1.5
    high_exp = 1.5
    points   = 20
    # Plot `points` points from 1^(`low_exp`) to 1^(`high_exp`) 

    clf_face_rbf_pca = GridSearchCV(SVC(C=1), {'C':np.logspace(low_exp, high_exp, num=points), 'kernel':['rbf']}, n_jobs=2)
    clf_face_rbf_pca.fit(X_train, y_train)

    C_params = [param['C'] for param in clf_face_rbf_pca.cv_results_['params']]
    error   = 100 - 100*clf_face_rbf_pca.cv_results_['mean_test_score']
#     print("i = %s, min error = %s" % (i, np.min(error)))
    if(np.min(error) < min_err):
        min_err = np.min(error)
        min_i = i
        min_c = clf_face_rbf_pca.best_params_['C']
        
print("\nLowest error found with when reducing dimension to %s features, with C = %s, error = %s%%" % (min_i, min_err, min_c))
